{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0500182ff3f2c5a01a35676a8566cb1c6ccd4991a8858442ee9695627a778d294",
   "display_name": "Python 3.9.1 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "663fcf70a6268f48f0ae5969ce6de9988ad359ea414e524dbf21a764de294fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import requests\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "from decouple import config\n",
    "from bs4 import BeautifulSoup\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:87.0) Gecko/20100101 Firefox/87.0'\n",
    "}\n",
    "azure_access_token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverso_request(start, length):\n",
    "    base_url = f\"https://context.reverso.net/bst-web-user/user/favourites/shared\"\n",
    "    \n",
    "    params = {\n",
    "        'userName': 'lmirandam07',\n",
    "        'start': start,\n",
    "        'length': length,\n",
    "        'order': 10\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        request = requests.get(base_url, params=params, headers=headers)\n",
    "        request.raise_for_status()\n",
    "        \n",
    "        return request.json()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop the duplicated words from the JSON\n",
    "def words_filter(words, csv_headers): \n",
    "    csv_file = Path(\"./words_list.csv\")\n",
    "    csv_file.touch(exist_ok=True)\n",
    "    empty_file = False\n",
    "\n",
    "    with open(csv_file, 'r') as file: \n",
    "      reader = list(csv.reader(file))\n",
    "\n",
    "      # If the file only has headers\n",
    "      if len(reader) == 1 or reader[2] != \"\\n\":\n",
    "        return words\n",
    "\n",
    "      # If the file is empty\n",
    "      if len(reader) < 1:\n",
    "        empty_file = True\n",
    "      elif len(reader) >=2:\n",
    "        # Get the last german word, but only the woird without article or plural\n",
    "        last_word_de = reader[-1][0].split(\" \")[1]\n",
    "\n",
    "    if empty_file:\n",
    "      with open(csv_file, 'w', encoding='utf-8', newline='') as file: \n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        writer.writerow(csv_headers.keys() + \"\\n\")\n",
    "        return words\n",
    "\n",
    "    last_updated_idx = words.index(list(filter(lambda w: w['srcText'] == last_word_de if w['srcLang'] == 'de' else w['trgText'] == last_word_de, words))[0])\n",
    "\n",
    "    return words[:last_updated_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(de_word):\n",
    "    tags = ('adjetivo', 'sustantivo', 'adverbio', 'verbo')\n",
    "    reverso_url = f\"https://context.reverso.net/traduccion/aleman-espanol/{de_word}\"\n",
    "\n",
    "    try:\n",
    "        req = requests.get(reverso_url, headers=headers)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        word_tag = soup.select(\"#pos-filters button\")[0].text.strip().lower() or \"\"\n",
    "\n",
    "        if word_tag not in tags:\n",
    "            print(de_word, word_tag)\n",
    "            return \"\"\n",
    "        \n",
    "        return word_tag\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_article(de_word):\n",
    "    leo_url = f\"https://dict.leo.org/alem%C3%A1n-espa%C3%B1ol/{de_word}\"\n",
    "\n",
    "    try:\n",
    "        req = requests.get(leo_url, headers=headers)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")        \n",
    "        de_noun = soup.select(\"#section-subst td[lang='de'] samp\")\n",
    "\n",
    "        de_article = de_noun[0].text.split(' ')[0] or ''\n",
    "        de_plural = de_noun[0].find('small').text or ''\n",
    "        de_word = f\"{de_article} {de_word} - {de_plural}\"\n",
    "        return de_word\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(sub_key, region):\n",
    "    fetch_token_url = f\"https://{region}.api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': sub_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(fetch_token_url, headers=headers)\n",
    "        return str(response.text)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_audio(de_sentence):\n",
    "    AZURE_API_KEY = config('AZURE_API_KEY')\n",
    "    AZURE_REGION = config('AZURE_REGION')\n",
    "    global azure_access_token\n",
    "\n",
    "    if not azure_access_token:\n",
    "        azure_access_token = get_access_token(AZURE_API_KEY, AZURE_REGION)\n",
    "\n",
    "    try:\n",
    "        langs_and_voices = {\n",
    "            'de-DE': ('de-DE-ConradNeural', 'de-DE-KatjaNeural'),\n",
    "            'de-AT': ('de-AT-JonasNeural', 'de-AT-IngridNeural'),\n",
    "            'de-CH': ('de-CH-LeniNeural', 'de-CH-JanNeural')\n",
    "        }\n",
    "        # From the list of voices in german in the API, randomly select one\n",
    "        lang_choice = choice(list(langs_and_voices.keys()))\n",
    "        voice_choice = choice(langs_and_voices[lang_choice])\n",
    "\n",
    "        rate = 0\n",
    "        pitch = 0\n",
    "        \n",
    "        azure_api_url = f'https://{AZURE_REGION}.tts.speech.microsoft.com/cognitiveservices/v1'\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {azure_access_token}',\n",
    "            'Content-Type': 'application/ssml+xml',\n",
    "            'X-Microsoft-OutputFormat': 'audio-24khz-96kbitrate-mono-mp3',\n",
    "            'User-Agent': 'reverso-anki-automation'\n",
    "        }\n",
    "        # Create XML format that uses the API to make the translation\n",
    "        xml_body = ElementTree.Element('speak', version='1.0')\n",
    "        xml_body.set('{http://www.w3.org/XML/1998/namespace}lang', lang_choice)\n",
    "\n",
    "        voice = ElementTree.SubElement(xml_body, 'voice')\n",
    "        voice.set('{http://www.w3.org/XML/1998/namespace}lang', lang_choice)\n",
    "        voice.set(\n",
    "            'name', voice_choice)\n",
    "\n",
    "        prosody = ElementTree.SubElement(voice, 'prosody')\n",
    "        prosody.set('rate', f'{rate}%')\n",
    "        prosody.set('pitch', f'{pitch}%')\n",
    "        prosody.text = de_sentence\n",
    "\n",
    "        body = ElementTree.tostring(xml_body)\n",
    "\n",
    "        response = requests.post(azure_api_url, headers=headers, data=body)\n",
    "\n",
    "        if response.status_code in range(200,300):\n",
    "            audio_folder = Path(\"./audios\")\n",
    "            audio_folder.mkdir(exist_ok=True)\n",
    "            audio_file_name = Path(f\"azure-{str(uuid4())}.mp3\")\n",
    "            audio_file_path = Path.joinpath(audio_folder, audio_file_name)\n",
    "\n",
    "            if not audio_file_path.exists():\n",
    "                with open(audio_file_path, 'wb') as audio:\n",
    "                    audio.write(response.content)\n",
    "\n",
    "                return audio_file_name.name\n",
    "\n",
    "        return ''\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_list(word, words_dict):\n",
    "    words_dict_c = words_dict.copy()\n",
    "    # If src lang is german, then return the first order otherwise change the order\n",
    "    language_order = lambda lang: ('src', 'trg') if lang =='de' else ('trg', 'src')\n",
    "\n",
    "    de_order, es_order = language_order(word['srcLang'])\n",
    "    words_dict_c['de_word'] = word[f'{de_order}Text']\n",
    "    words_dict_c['es_word'] = word[f'{es_order}Text']\n",
    "    # To remove <em> tags \n",
    "    words_dict_c['de_sentence'] = BeautifulSoup(word[f'{de_order}Context'], features=\"html.parser\").get_text()\n",
    "    words_dict_c['es_sentence'] = BeautifulSoup(word[f'{es_order}Context'], features=\"html.parser\").get_text()\n",
    "\n",
    "    words_dict_c['tags'] = get_word_tag(words_dict_c['de_word'])\n",
    "\n",
    "    if words_dict_c['tags'] == 'sustantivo':\n",
    "        words_dict_c['de_word'] = get_noun_article(words_dict_c['de_word'])\n",
    "\n",
    "    words_dict_c['de_audio'] = get_sentence_audio(words_dict_c['de_sentence'])\n",
    "    return words_dict_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_csv(words_list):\n",
    "    csv_file = Path(\"./words_list.csv\")\n",
    "    with open(csv_file, 'a', encoding='utf-8', newline='') as file: \n",
    "      writer = csv.DictWriter(file, fieldnames=words_list[0].keys())\n",
    "      for w in words_list:\n",
    "        writer.writerow(w) \n",
    "    # TODO: Loggear la cantidad de palabras que se han añadido nuevas\n",
    "    print(f'Se ha completado el proceso correctamente, se han añadido {len(words_list)} palabras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # logging.basicConfig(filename='scraper.log', level=logging.INFO)\n",
    "    words_dict = {\n",
    "        'de_word': '',\n",
    "        'de_sentence': '',\n",
    "        'es_word': '',\n",
    "        'es_sentence': '',\n",
    "        'de_audio': '',\n",
    "        'tags': ''\n",
    "    }\n",
    "    start = 0\n",
    "    length = 10\n",
    "    data = reverso_request(start, length)\n",
    "    words_results = data['results']\n",
    "    num_total_results = data['numTotalResults']\n",
    "    if num_total_results > length:\n",
    "        # Starts the requests in the end of the previus and make another with all the remaining words\n",
    "        start = length\n",
    "        length = num_total_results - length\n",
    "        data = reverso_request(start, length)\n",
    "        words_results.extend(data['results'])\n",
    "\n",
    "    filtered_words = words_filter(words_results, words_dict)\n",
    "\n",
    "    words_dict_list = [get_words_list(f_w, words_dict) for f_w in filtered_words[:7]]\n",
    "    \n",
    "    save_words_csv(words_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9d2ec3fd83d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m    \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-1e7f8fd5c39b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mwords_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mfiltered_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mwords_dict_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_words_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_dict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf_w\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-46114ba4e542>\u001b[0m in \u001b[0;36mwords_filter\u001b[1;34m(words, csv_headers)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m       \u001b[1;31m# If the file only has headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   main()"
   ]
  }
 ]
}